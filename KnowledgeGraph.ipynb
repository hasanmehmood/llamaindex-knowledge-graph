{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index import (\n",
    "    KnowledgeGraphIndex,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# define LLM\n",
    "# NOTE: at the time of demo, text-davinci-002 did not have rate-limit errors\n",
    "llm = OpenAI(temperature=0, model=\"text-davinci-002\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a NebulaGraph (version 3.5.0 or newer) cluster with:\n",
    "# Option 0 for machines with Docker: `curl -fsSL nebula-up.siwei.io/install.sh | bash`\n",
    "# Option 1 for Desktop: NebulaGraph Docker Extension https://hub.docker.com/extensions/weygu/nebulagraph-dd-ext\n",
    "\n",
    "# If not, create it with the following commands from NebulaGraph's console:\n",
    "# CREATE SPACE llamaindex(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    "# :sleep 10;\n",
    "# USE llamaindex;\n",
    "# CREATE TAG entity(name string);\n",
    "# CREATE EDGE relationship(relationship string);\n",
    "# :sleep 10;\n",
    "# CREATE TAG INDEX entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a NebulaGraph Cluster (Version 3.5.0 or Newer)\n",
    "\n",
    "## Using Docker (Machines with Docker Installed)\n",
    "Option 0: Run the following command:\n",
    "```bash\n",
    "curl -fsSL nebula-up.siwei.io/install.sh | bash\n",
    "```\n",
    "\n",
    "## Desktop Installation\n",
    "Option 1: Install NebulaGraph Docker Extension from [Docker Hub](https://hub.docker.com/extensions/weygu/nebulagraph-dd-ext).\n",
    "\n",
    "## Manual Setup via NebulaGraph Console\n",
    "If the above options are not applicable, manually create the NebulaGraph space, tags, edges, and indexes using the NebulaGraph console:\n",
    "\n",
    "1. Create the space named `llamaindex`:\n",
    "```plaintext\n",
    "CREATE SPACE llamaindex(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE llamaindex;\n",
    "```\n",
    "\n",
    "2. Define a tag named `entity` with a property named `name`:\n",
    "```plaintext\n",
    "CREATE TAG entity(name string);\n",
    "```\n",
    "\n",
    "3. Define an edge named `relationship`:\n",
    "```plaintext\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "```\n",
    "\n",
    "4. Create an index for the `name` property of the `entity` tag:\n",
    "```plaintext\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```\n",
    "\n",
    "Adjust the parameters (such as `vid_type`, `partition_num`, `replica_factor`, etc.) based on your specific requirements or environment setup.\n",
    "```\n",
    "\n",
    "This Markdown structure provides a step-by-step guide for creating a NebulaGraph cluster using Docker or manual setup via the NebulaGraph console. Adjust the content and formatting as needed for your documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEBULA_USER\"] = \"root\"\n",
    "os.environ[\"NEBULA_PASSWORD\"] = \"nebula\"  # default is \"nebula\"\n",
    "os.environ[\n",
    "    \"NEBULA_ADDRESS\"\n",
    "] = \"127.0.0.1:9669\"  # assumed we have NebulaGraph installed locally\n",
    "\n",
    "space_name = \"llamaindex\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\n",
    "    \"relationship\"\n",
    "]  # default, could be omit if create from an empty kg\n",
    "tags = [\"entity\"]  # default, could be omit if create from an empty kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    service_context=service_context,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = KnowledgeGraphQueryEngine(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"you question on the data?\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
